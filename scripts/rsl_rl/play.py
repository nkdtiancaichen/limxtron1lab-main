"""RSL-RLæ™ºèƒ½ä½“æ£€æŸ¥ç‚¹æ’­æ”¾è„šæœ¬ / Script to play a checkpoint of an RL agent from RSL-RL."""

"""é¦–å…ˆå¯åŠ¨Isaac Simä»¿çœŸå™¨ / Launch Isaac Sim Simulator first."""

import argparse
import copy  # [æ–°å¢] ç”¨äºå¤åˆ¶ç¥ç»ç½‘ç»œç­–ç•¥

from isaaclab.app import AppLauncher

# local imports
import cli_args  # isort: skip

# æ·»åŠ argparseå‚æ•° / Add argparse arguments
parser = argparse.ArgumentParser(description="Train an RL agent with RSL-RL.")
parser.add_argument("--video", action="store_true", default=False, help="Record videos during training.")
parser.add_argument("--video_length", type=int, default=200, help="Length of the recorded video (in steps).")
parser.add_argument(
    "--disable_fabric", action="store_true", default=False, help="Disable fabric and use USD I/O operations."
)
parser.add_argument("--num_envs", type=int, default=None, help="Number of environments to simulate.")
parser.add_argument("--task", type=str, default=None, help="Name of the task.")
parser.add_argument("--seed", type=int, default=None, help="Seed used for the environment")
parser.add_argument("--checkpoint_path", type=str, default=None, help="Relative path to checkpoint file.")
parser.add_argument("--hop_checkpoint_path", type=str, default=None, help="Path to the hopping/stunt model checkpoint file.")

# append RSL-RL cli arguments
cli_args.add_rsl_rl_args(parser)
# append AppLauncher cli args
AppLauncher.add_app_launcher_args(parser)
args_cli = parser.parse_args()

if args_cli.video:
    args_cli.enable_cameras = True

# launch omniverse app
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

"""Rest everything follows."""


import gymnasium as gym
import os
import torch

from rsl_rl.runner import OnPolicyRunner

from isaaclab.envs import ManagerBasedRLEnvCfg,DirectMARLEnv, multi_agent_to_single_agent
from isaaclab.utils.dict import print_dict
from isaaclab_tasks.utils import get_checkpoint_path, parse_env_cfg
from isaaclab_rl.rsl_rl import RslRlOnPolicyRunnerCfg, RslRlVecEnvWrapper
# Import extensions to set up environment tasks
import bipedal_locomotion  # noqa: F401
from bipedal_locomotion.utils.wrappers.rsl_rl import RslRlPpoAlgorithmMlpCfg, export_mlp_as_onnx, export_policy_as_jit

# åœ¨æ–‡ä»¶æœ€ä¸Šæ–¹æ·»åŠ 
import sys
from pynput import keyboard
import threading


# --- é”®ç›˜æ§åˆ¶å™¨ç±» ---
# --- é”®ç›˜æ§åˆ¶å™¨ç±» (å®Œæ•´ä¿®æ­£ç‰ˆ) ---
class KeyboardInterface:
    def __init__(self):
        self.vx = 0.0  # å‰åé€Ÿåº¦
        self.vy = 0.0  # å·¦å³å¹³ç§»
        self.wz = 0.0  # æ—‹è½¬é€Ÿåº¦

        # ğŸ‘‡ğŸ‘‡ğŸ‘‡ [å…³é”®ä¿®æ­£] å¿…é¡»åœ¨è¿™é‡Œåˆå§‹åŒ–è¿™ä¸ªå˜é‡ï¼
        self.use_hopping_policy = False

        # å¯åŠ¨ç›‘å¬çº¿ç¨‹
        self.listener = keyboard.Listener(on_press=self.on_press)
        self.listener.start()

        print("\n" + "=" * 30)
        print("  é”®ç›˜æ§åˆ¶å·²æ¿€æ´» (Keyboard Active)")
        print("  W / S : å‰è¿› / åé€€ (Vx)")
        print("  A / D : å·¦ç§» / å³ç§» (Vy)")
        print("  Q / E : å·¦è½¬ / å³è½¬ (Yaw)")
        print("  Space : æ€¥åœ (Stop)")
        print("  K     : åˆ‡æ¢è¡Œèµ°/å•è„šè·³æ¨¡å¼ (Toggle Mode)")  # [æ–°å¢] æç¤º
        print("=" * 30 + "\n")

    def on_press(self, key):
        try:
            # æ­¥é•¿è®¾ç½®
            step_lin = 0.1
            step_ang = 0.1

            if hasattr(key, 'char'):
                if key.char == 'w':
                    self.vx += step_lin
                elif key.char == 's':
                    self.vx -= step_lin
                elif key.char == 'a':
                    self.vy += step_lin
                elif key.char == 'd':
                    self.vy -= step_lin
                elif key.char == 'q':
                    self.wz += step_ang
                elif key.char == 'e':
                    self.wz -= step_ang
                elif key.char == ' ':
                    self.vx, self.vy, self.wz = 0, 0, 0

                # ğŸ‘‡ğŸ‘‡ğŸ‘‡ [å…³é”®ä¿®æ­£] Ké”®åˆ‡æ¢é€»è¾‘
                elif key.char == 'k':
                    self.use_hopping_policy = not self.use_hopping_policy
                    mode_str = "ã€å•è„šè·³ HOPPINGã€‘" if self.use_hopping_policy else "ã€æ­£å¸¸è¡Œèµ° WALKINGã€‘"
                    # åŠ  \r ç¡®ä¿æ‰“å°ä¸ä¹±è¡Œ
                    print(f"\r>>> åˆ‡æ¢æ¨¡å¼: {mode_str}                  ")

            # é™å¹…
            self.vx = max(min(self.vx, 1.5), -1.0)
            self.vy = max(min(self.vy, 0.5), -0.5)
            self.wz = max(min(self.wz, 1.5), -1.5)

            # å®æ—¶æ‰“å°å½“å‰æŒ‡ä»¤
            sys.stdout.write(f"\r[Cmd] Vx: {self.vx:.2f} | Vy: {self.vy:.2f} | Wz: {self.wz:.2f}   ")
            sys.stdout.flush()

        except AttributeError:
            pass


# --------------------

def main():
    """ä½¿ç”¨RSL-RLæ™ºèƒ½ä½“è¿›è¡Œæµ‹è¯• / Play with RSL-RL agent."""

    # 1. åœ¨è¿™é‡Œåˆå§‹åŒ–é”®ç›˜æ§åˆ¶å™¨ (åœ¨ gym.make æˆ– envåˆ›å»ºå‰åå‡å¯)
    keyboard_cmd = KeyboardInterface()

    # è§£æé…ç½® / Parse configuration
    env_cfg: ManagerBasedRLEnvCfg = parse_env_cfg(
        task_name=args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs
    )
    agent_cfg: RslRlPpoAlgorithmMlpCfg = cli_args.parse_rsl_rl_cfg(args_cli.task, args_cli)

    env_cfg.seed = agent_cfg.seed

    # æŒ‡å®šæ—¥å¿—å®éªŒç›®å½• / Specify directory for logging experiments
    if args_cli.checkpoint_path is None:
        log_root_path = os.path.join("logs", "rsl_rl", agent_cfg.experiment_name)
        log_root_path = os.path.abspath(log_root_path)
        print(f"[INFO] Loading experiment from directory: {log_root_path}")
        resume_path = get_checkpoint_path(log_root_path, agent_cfg.load_run, agent_cfg.load_checkpoint)
    else:
        resume_path = args_cli.checkpoint_path
    log_dir = os.path.dirname(resume_path)

    # åˆ›å»ºisaacç¯å¢ƒ / Create isaac environment
    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)

    if args_cli.video:
        video_kwargs = {
            "video_folder": os.path.join(log_dir, "videos", "play"),
            "step_trigger": lambda step: step == 0,
            "video_length": args_cli.video_length,
            "disable_logger": True,
        }
        print("[INFO] Recording videos during training.")
        print_dict(video_kwargs, nesting=4)
        env = gym.wrappers.RecordVideo(env, **video_kwargs)

    # convert to single-agent instance if required by the RL algorithm
    if isinstance(env.unwrapped, DirectMARLEnv):
        env = multi_agent_to_single_agent(env)

    # wrap around environment for rsl-rl
    env = RslRlVecEnvWrapper(env)
    # load previously trained model
    print(f"[INFO]: Loading model checkpoint from: {resume_path}")
    ppo_runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=None, device=agent_cfg.device)
    walk_checkpoint = resume_path
    print(f"[INFO]: Loading WALKING model from: {walk_checkpoint}")
    ppo_runner.load(walk_checkpoint)

    # æ·±æ‹·è´ä¿å­˜è¡Œèµ°ç­–ç•¥
    policy_walk = copy.deepcopy(ppo_runner.get_inference_policy(device=env.unwrapped.device))
    encoder_walk = copy.deepcopy(ppo_runner.get_inference_encoder(device=env.unwrapped.device))

    hop_checkpoint = args_cli.hop_checkpoint_path
    if hop_checkpoint and os.path.exists(hop_checkpoint):
        print(f"\n[INFO]: Loading HOPPING model from: {hop_checkpoint}")
        # åŠ è½½å•è„šè·³æƒé‡
        ppo_runner.load(hop_checkpoint)

        # æ·±æ‹·è´ä¿å­˜å•è„šè·³ç­–ç•¥
        policy_hop = copy.deepcopy(ppo_runner.get_inference_policy(device=env.unwrapped.device))
        encoder_hop = copy.deepcopy(ppo_runner.get_inference_encoder(device=env.unwrapped.device))
        print("[SUCCESS] Hopping policy loaded successfully. Press 'K' to toggle.\n")
    else:
        if hop_checkpoint:
            print(f"[WARNING]: æŒ‡å®šçš„å•è„šè·³æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {hop_checkpoint}")
        else:
            print("[INFO]: æœªæŒ‡å®šå•è„šè·³æ¨¡å‹ (--hop_checkpoint_path)ï¼ŒKé”®åˆ‡æ¢å°†æ— æ•ˆã€‚")

        # å¦‚æœæ²¡ä¼ å‚æ•°æˆ–è·¯å¾„ä¸å¯¹ï¼Œé»˜è®¤å›é€€åˆ°è¡Œèµ°ç­–ç•¥ï¼Œé˜²æ­¢æŠ¥é”™
        policy_hop = policy_walk
        encoder_hop = encoder_walk

     # å¯¼å‡ºç­–ç•¥åˆ°onnx / Export policy to onnx
    if EXPORT_POLICY:
        export_model_dir = os.path.join(os.path.dirname(resume_path), "exported")
        export_policy_as_jit(
            ppo_runner.alg.actor_critic, export_model_dir
        )
        print("Exported policy as jit script to: ", export_model_dir)
        export_mlp_as_onnx(
            ppo_runner.alg.actor_critic.actor, 
            export_model_dir, 
            "policy",
            ppo_runner.alg.actor_critic.num_actor_obs,
        )
        export_mlp_as_onnx(
            ppo_runner.alg.encoder,
            export_model_dir,
            "encoder",
            ppo_runner.alg.encoder.num_input_dim,
        )
    # reset environment
    # -------- ä¿®å¤ 1: åˆå§‹åŒ– get_observations (æ„é€ åµŒå¥—ç»“æ„) --------
    returns = env.get_observations()

    # åŠ¨æ€å¤„ç† TensorDict
    if str(type(returns)).find("TensorDict") != -1 or isinstance(returns, dict):
        # 1. æå– Policy è¾“å…¥ (obs)
        if "policy" in returns.keys():
            obs = returns["policy"]
        elif "obs" in returns.keys():
            obs = returns["obs"]
        else:
            obs = returns

        # 2. æ„é€ æ—§ä»£ç æœŸæœ›çš„åµŒå¥—å­—å…¸ {"observations": {...}}
        # è¿™ä¸€æ­¥æ˜¯è§£å†³ KeyError çš„å…³é”®ï¼
        obs_dict = {"observations": {}}

        # è‡ªåŠ¨æ¬è¿æ‰€æœ‰é”®å€¼åˆ° observations ä¸‹
        # å°¤å…¶æ˜¯ obsHistory, commands, critic
        for key in returns.keys():
            # ç»Ÿä¸€å¤„ç†é”®å (æ¯”å¦‚æŠŠ obs_history æ˜ å°„å› obsHistory)
            if key == "obs_history":
                obs_dict["observations"]["obsHistory"] = returns[key]
            else:
                obs_dict["observations"][key] = returns[key]

    elif isinstance(returns, tuple):
        # æ—§ç‰ˆå…¼å®¹
        if len(returns) == 3:
            obs, privileged_obs, obs_dict = returns
        else:
            obs, obs_dict = returns
    else:
        obs = returns
        obs_dict = {"observations": {}}
    # -----------------------------------------------------------
    obs_history = obs_dict["observations"].get("obsHistory")
    obs_history = obs_history.flatten(start_dim=1)
    commands = obs_dict["observations"].get("commands") 
    # simulate environment
    while simulation_app.is_running():
        # run everything in inference mode
        with torch.inference_mode():
            # 1. ğŸ‘‡ å…ˆå†³å®šç”¨å“ªä¸ªæ¨¡å‹ (è¿™å—ä»£ç ä½ å¯èƒ½å·²ç»åŠ äº†)
            if keyboard_cmd.use_hopping_policy:
                current_policy = policy_hop
                current_encoder = encoder_hop
            else:
                current_policy = policy_walk
                current_encoder = encoder_walk

            # agent stepping
            est = current_encoder(obs_history)
            actions = current_policy(torch.cat((est, obs, commands), dim=-1).detach())
            # env stepping
            # -------- æœ€ç»ˆä¿®å¤: å˜é‡åä¿®æ­£ä¸º infos --------
            step_returns = env.step(actions)

            # 1. è§£åŒ… (å˜é‡åå¿…é¡»å« infosï¼Œé…åˆåé¢çš„ä»£ç )
            if len(step_returns) == 5:
                # Isaac Lab 0.47+: obs, privileged_obs, rewards, dones, infos
                obs, privileged_obs, rewards, dones, infos = step_returns
            elif len(step_returns) == 4:
                # æ—§ç‰ˆæ ‡å‡†: obs, rewards, dones, infos
                obs, rewards, dones, infos = step_returns
            else:
                obs = step_returns[0]
                infos = {}  # é˜²æ­¢å´©æºƒ

            # 2. å¤„ç† TensorDict -> å¡«å…… infos
            if str(type(obs)).find("TensorDict") != -1 or isinstance(obs, dict):
                full_dict = obs

                # A. æå– Policy è¾“å…¥ (çœŸæ­£å–‚ç»™ç¥ç»ç½‘ç»œçš„ obs)
                if "policy" in full_dict.keys():
                    obs = full_dict["policy"]
                elif "obs" in full_dict.keys():
                    obs = full_dict["obs"]

                # B. æ¬è¿æ•°æ®åˆ° infos (å› ä¸ºåé¢çš„ä»£ç ä» infos é‡Œå– obsHistory)
                # ç¡®ä¿ infos["observations"] å­˜åœ¨
                if "observations" not in infos:
                    infos["observations"] = {}

                # å°† TensorDict é‡Œçš„æ‰€æœ‰æ•°æ®ï¼ˆå¦‚ obsHistory, commandsï¼‰æ¬è¿è¿›å»
                for key in full_dict.keys():
                    if key == "obs_history":
                        infos["observations"]["obsHistory"] = full_dict[key]
                    else:
                        infos["observations"][key] = full_dict[key]

            # 3. ä¿åº•æ£€æŸ¥
            if "observations" not in infos:
                infos["observations"] = {}
            # -----------------------------------------------------
            obs_history = infos["observations"].get("obsHistory")
            obs_history = obs_history.flatten(start_dim=1)
            commands = infos["observations"].get("commands")

            # ============ ã€å…³é”®ä¿®æ”¹ã€‘å¼ºè¡Œè¦†ç›– Commands ============
            # ====================================================
            if commands is not None:
                # commands çš„å½¢çŠ¶é€šå¸¸æ˜¯ [num_envs, 3] -> (vx, vy, wz)
                # æ— è®ºæœ‰å¤šå°‘ä¸ªç¯å¢ƒï¼Œæˆ‘ä»¬æŠŠæ‰€æœ‰æœºå™¨äººçš„æŒ‡ä»¤éƒ½è®¾ä¸ºé”®ç›˜æ§åˆ¶çš„å€¼

                # è¦†ç›– X è½´çº¿é€Ÿåº¦ (Vx)
                commands[:, 0] = keyboard_cmd.vx

                # è¦†ç›– Y è½´çº¿é€Ÿåº¦ (Vy)
                commands[:, 1] = keyboard_cmd.vy

                # è¦†ç›– Z è½´è§’é€Ÿåº¦ (Yaw)
                commands[:, 2] = keyboard_cmd.wz

                # (å¯é€‰) å¦‚æœä½ çš„æŒ‡ä»¤åŒ…å«æ›´å¤šç»´åº¦ (å¦‚é«˜åº¦ã€é¢‘ç‡)ï¼Œä¿æŒåŸæ ·æˆ–æ‰‹åŠ¨è®¾ç½®
                # commands[:, 3] = ...
            else:
                # å¦‚æœç¯å¢ƒç”šè‡³æ²¡è¿”å› commandsï¼Œæˆ‘ä»¬æ‰‹åŠ¨é€ ä¸€ä¸ª
                # è¿™åœ¨æŸäº›æç«¯æƒ…å†µä¸‹æ˜¯å¿…è¦çš„é˜²å´©æºƒæªæ–½
                commands = torch.zeros((env.num_envs, 3), device=env.device)
                commands[:, 0] = keyboard_cmd.vx
                commands[:, 1] = keyboard_cmd.vy
                commands[:, 2] = keyboard_cmd.wz
            # ====================================================

    # close the simulator
    env.close()


if __name__ == "__main__":
    EXPORT_POLICY = True
    # run the main execution
    main()
    # close sim app
    simulation_app.close()
